{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "", ""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('outputs/res_with__emojis.csv')\n",
    "texts = data['text'].tolist()\n",
    "emoji = data['lang'].tolist()\n",
    "no_emoji = pd.read_csv('outputs/res_with_no_emojis.csv')['lang'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking differences between emoji and no emoji texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ur : th : ğŸ‘¹ğŸ‘º\n",
      "ur : hi : Sure dine ğŸ˜ŠğŸ˜ŠğŸ˜ğŸ˜ğŸ˜ğŸ˜ğŸ˜›ğŸ˜›ğŸ˜›ğŸ˜›ğŸ¦–ğŸ¦–ğŸ¦–ğŸ¦–ğŸ˜ˆğŸ˜ˆğŸ˜ˆğŸ˜ˆğŸ˜ˆğŸ™ŠğŸ™ŠğŸ™ŠğŸ™ŠğŸ™ŠğŸ™ŠğŸ™ŠğŸ™ŠğŸ™ŠğŸ™Š\n"
     ]
    }
   ],
   "source": [
    "for text, emoji_lang, no_emoji_lang in zip(texts, emoji, no_emoji):\n",
    "    if emoji_lang != no_emoji_lang:\n",
    "        print(f'{emoji_lang} : {no_emoji_lang} : {text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between texts with emojis and without emojis only occurs in two texts where there are many of these emojis, compared to the amount of text. Therefore, further analysis will not focus on this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## False prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>en</td>\n",
       "      <td>Where the Nova Scotia government is typically ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>es</td>\n",
       "      <td>Era consultarle . PensÃ© hacerla para Argentina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>en</td>\n",
       "      <td>Will the procurement plan be digitalized, or w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>en</td>\n",
       "      <td>Why can't fundamental Church teachings just be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ja</td>\n",
       "      <td>çµ†å‰µè†</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>it</td>\n",
       "      <td>L</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>en</td>\n",
       "      <td>We need employees in all departments and at va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>pl</td>\n",
       "      <td>....to own challenges w solutions/ recommendat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>en</td>\n",
       "      <td>leverage IR&amp;D dollars: needs increased interac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>en</td>\n",
       "      <td>Any plans for PB RRMs reviewing credit request...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lang                                               text\n",
       "0   en  Where the Nova Scotia government is typically ...\n",
       "1   es  Era consultarle . PensÃ© hacerla para Argentina...\n",
       "2   en  Will the procurement plan be digitalized, or w...\n",
       "3   en  Why can't fundamental Church teachings just be...\n",
       "4   ja                                                çµ†å‰µè†\n",
       "5   it                                                  L\n",
       "6   en  We need employees in all departments and at va...\n",
       "7   pl  ....to own challenges w solutions/ recommendat...\n",
       "8   en  leverage IR&D dollars: needs increased interac...\n",
       "9   en  Any plans for PB RRMs reviewing credit request..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Case 1 ``` pl\t- ....to own challenges w solutions/ recommendations & implement. (2/2) ```\n",
    "2. Case 2 ``` ur - \tNo ```\n",
    "3. Case 3 ```hi\t- fuck aidan ```\n",
    "4. Case 4 ``` hi -\t5519290```\n",
    "5. Case 5 ``` hi -\thttps://editor.p5js.org/jbluth22/sketches/Ah8R...```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "language_detector = pipeline(model='papluca/xlm-roberta-base-language-detection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'pl', 'score': 0.9406532049179077}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_detector('....to own challenges w solutions/ recommendations & implement. (2/2)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'en', 'score': 0.5793170928955078}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_detector('to own challenges w solutions/ recommendations & implement. (2/2)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first case, we can see that the text was incorrectly classified as Polish. However, further analysis shows that removing the dots at the beginning of the sentence corrects the issue. This indicates that the model is not resilient in this case to noise caused by punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second case, it is uncertain whether the prediction was accurate or not; it was likely supposed to be English. However, the word 'No' itself could be correct for multiple languages. For instance, in Polish, the word 'no' is commonly used as a confirmation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third case, we see that an English expression was incorrectly classified as Hindi. This may be due to the absence of offensive texts in the training dataset, causing the classifier to make a random selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the fourth and fifth cases, we see that these texts do not have a clear language affiliation, yet they have been assigned to the Hindi language. It is likely that such expressions were not present in the training dataset. Taken together with the third case, we can infer that cases that may not have been present in the training dataset, or those that the model is uncertain about due to their specificity, are classified as Hindi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The consequences of these errors could result in incorrect predictions in subsequent processing stages, even if the next model is perfect. Let's imagine that the model selection in the second stage depends on the detected language. So, even if we have a model in English with 100% accuracy, this information might be passed on to the German model, further propagating the errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final conclusion is that the model has generally performed well in assigning languages (in cases where I can personally distinguish English, Polish, German, and Spanish). However, the analysis has also shown that the model incorrectly assigns labels to certain texts, such as offensive language. One of the reasons for this behavior could be the absence of such texts in the training corpus. If this were true, it would violate one of the basic principles of NLP, which states that the corpus should be as representative as possible for a given language. This is a significant drawback of this model, as offensive texts are often present in social media. Another drawback of the model is the limited number of languages it supports (only 20). Therefore, when working with less common languages, the model would be of no use. On the positive side, the model exhibits high accuracy in general texts, as demonstrated by the results from the publication. In this case, what could help? As shown in example number 1, a higher degree of data cleaning could lead to better model results. However, this cleaning cannot go too far because LLMs are trained on natural language, so, for example, removing stop words would be too much interference. Therefore, a more careful and selective text cleaning to a limited extent would be useful. Of course, as I mentioned earlier, it also depends a lot on the data on which the model was trained, whether it saw "strange punctuation constructions," errors, or only clean expressions in a given language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
